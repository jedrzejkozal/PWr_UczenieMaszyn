\documentclass[a4paper, 10 pt, journal]{ieeeconf}
\overrideIEEEmargins
\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel} 
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{url}
\usepackage{makecell}
\usepackage{color, colortbl}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{gray!5!white},   
	commentstyle=\color{green!50!black},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{black!50!white},
	stringstyle=\color{purple},
	basicstyle=\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}
\lstset{style=mystyle}

\usepackage{tikz}

\newcommand{\bb}{\textbf}
\DeclareMathOperator*{\argmax}{argmax}
\definecolor{Gray}{gray}{0.9}

\title{\LARGE \bf
Methods of feature extraction for broad and deep datasets
}

\author{ \parbox{2 in}{\centering Filip Guzy \\
        Wrocław University of Science and Technology\\
        {\tt\small 218672@student.pwr.edu.pl}}
        \hspace*{ 0.3 in}
        \parbox{2 in}{\centering Jędrzej Kozal \\
        Wrocław University of Science and Technology\\
        {\tt\small 218557@student.pwr.edu.pl}}
}

\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\selectlanguage{english}
\begin{abstract}
Feature extraction is an important element in data preprocessing and it is commonly used in different areas of data science. This paper focuses on comparison of three different feature extraction algorithms, such as convolutional neural networks, principal component analysis and linear discriminant analysis. Methods were trained with 12 databases, evaluated using 10-fold stratified cross validation and compared by two classifiers - multilayer perceptron and support vector classifier. Experimental results showed that the performance of convolutional neural networks is the best from all selected methods.
\end{abstract}

\begin{keywords}
Feature extraction (FE), principal component analysis (PCA), linear discriminant analysis (LDA), convolutional neural networks (CNN).
\end{keywords}

\section{Introduction}

Feature extraction (FE) is the fundamental part of machine learning. Its main responsibility is to make sure that our classifiers won't get tons of unnecessary data on their inputs. We can divide FE algorithms into two groups: statistical and non-statistical. The group of statistical methods contains algorithms like principle component analysis (PCA) or linear discriminant analysis (LDA). In the second group we can find methods based on artificial intelligence, such as convolutional neural networks (CNN). 

Extracting particular features may help classifiers in making proper decisions, but in many cases it is not helpful for humans. When decision system is a black box, experts cannot use their intuition to predict the output, they also have no explanation why such decision was made. For example, it cannot be deduced why hospital diagnostic system assigned particular set of symptoms to the disease, or why loan request was rejected by the bank system.

FE methods can be straightly connected with the various fields of studies and analysed data. Therefore, many algorithms are composed in order to give the best performance for a specific domain or particular type of data. With regard to this fact, in this article both basic FE methods and state-of-the-art solutions will be analysed and compared. The main focus will be put on the comparison of PCA, LDA, and CNN approaches by applying them to 12 datasets and testing their effectiveness on 2 selected classifiers.

\section{Related work}
PCA was described for the first time in \cite{PCA}. Currently it is one of the most popular FE algorithms. It can be applied to many fields, especially to computer vision \cite{PCA_book}. Methods such as Robust PCA, Simplified PCA or Functional PCA were described in \cite{PCA_recent}. More insight into Robust PCA was given in \cite{RPCA}.

LDA was introduced in \cite{LDA} as a solution for a binary classification problem. Important adjustments, that enabled usage of LDA for multi-class problems, were made in \cite{multi_LDA}.  Authors of \cite{LDA_recent} focused mainly on the problem of LDA for small sample size data, which is not relevant in case of this work, but it gives a good overview on the research done recently in this field.

Balanced local discriminant embedding (BLDE) and convolutional neural networks as methods for spacial-domain and spectral-domain feature extraction were proposed in \cite{blde-cnn}. BLDE was used as a first part of the described hyperspectral images (HSI) classifier. Its task was to find a low-dimensional representation of classified images. Second part included CNN framework designed especially for extracting high level deep features. Extracted features were then stacked and input into logistic regression classifier. Usage of convolutional neural networks for feature extraction purposes was also proposed in \cite{cnn}. In this case, 3D CNN model with combined regularization was used in spectral-spacial feature extraction. Comparison of 3D CNN model with another deep learning based methods, like stacked autoencoder, deep brief network or 2D-CNN-based method was described in \cite{cnn2}. Another CNN-based deep learning approach was shown in \cite{cnn3}. Proposed approach assumed usage of the special type of CNN architecture - Deep Pyramidal Residual Network. The specific part of this architecture were pyramidal bottleneck residual blocks of convolutional layers. It allows to gradually increase the feature map dimension at all convolutional layers, what leads to involving more locations as the network depth increases, balancing the workload among all units and preserving the time complexity on each layer. Another solution for improving spectral-spatial feature extraction was proposed in \cite{cnn4}. Traditional pooling layer of CNN was replaced with spatial pyramid pooling, what resulted in increasement of extraction effectiveness.

A different approach, based on propagation filter (PF), was presented in \cite{propagation-filter}. First, the PCA algorithm was used in order to reduce HSI dimensionality. Then, the extracted principal components were filtered with the PF. In the last step of the experiment, extracted and filtered spectral-spatial features were input into support vector machine classifiers to test the effectiveness of the classification.

\section{Methods}

\subsection{Principle Component Analysis}
PCA allows to reduce dimensionality of the selected dataset. It results in maximising the variance of dataset by the new basis of lower dimensional space. Let's suppose that space $\mathbb{R}^{M}$ is given and we want to lower its dimension to $D$. We want to find projection matrix that enables this transformation of space. We also want to lose as little information as possible. In this case, using PCA requires calculation of eigenvalues and eigenvectors of covariance matrix $S$ (or evaluation of scatter matrix $A$, and calculating eigenvalues and eigenvectors of $\frac{1}{N}AA^{T}$ matrix). $D$ eigenvectors, that correspond to the biggest eigenvalues are used as a basis of a new space. These eigenvectors point the directions of the biggest change of variance in data.  The same problem can be reframed as maximalization of  a variance while projecting on vector $\bb{u}$:

\begin{align}
	f(\bb{u}) &= \bb{u}^T S \bb{u} \\
	g(\bb{u}) &= 1 - \bb{u}^T \bb{u} = 0   \notag
\end{align}

where $f$ is a variation while projecting on $\bb{u}$ and it is an additional constriction that yields $||\bb{u}||_{2} = 1$. The same can be formulated as Lagrangian:

\begin{align}
	\mathcal{L}(\bb{u}, \lambda) &= f(\bb{u}) - \lambda g(\bb{u}) \\
	    &= \bb{u}^T S \bb{u} - \lambda (1 - \bb{u}^T \bb{u})   \notag
\end{align}

and solved as optimization with the constrain problem. Please note that covariance matrix is symmetrical, i.e. it can be written as $S = Q \Lambda Q^{T}$, where $Q$ is an orthogonal matrix, and $\Lambda$ is a diagonal matrix with the real eigenvalues of $S$. Due to this fact finding a new basis, which is always orthogonal, is always possible.

Taking $\bb{u}_{n}$ as $n$-th eigenvector of covariance matrix, projection matrix $P$ can be written as:

\begin{align}
	P =
	\left( \begin{array}{l}
		\bb{u}_1^T \\
		\bb{u}_2^T \\
		\vdots	 \\
		\bb{u}_M^T
	\end{array} \right)
\end{align}

This matrix enables projection of any vector from $\mathbb{R}^M$ to $\mathbb{R}^D$.
More insight into the theory of PCA is given in \cite{Pattern_recognition}.

\subsection{Linear Discriminant Analysis}

LDA is a supervised dimensionality reduction algorithm, which aims to increase the linear separation of classes after projecting the original space. In order to achieve this goal the Fischer criterion is used:

\begin{equation}
    \argmax_{W} \frac{W^{T} S_{B} W}{W^{T} S_{W} W}
    \label{eq:Fischer_criterion}
\end{equation}

where $S_{B}$ is a between-class covariance, and $S_{W}$ is a within-class covariance. $W$ is a space that we are projecting on it. $S_{B}$ is defined as:

\begin{equation}
    S_B = \sum_{i} (\mu_{i} - \mu)(\mu_{i} - \mu)^{T}
\end{equation}

where $\mu_i = \frac{1}{n_{c_i}} \sum_{j=1}^{n_{c_j}} x_i$ is an average value of samples from $i$-th class, and $\mu = \frac{1}{N} \sum_{j=1}^{N} x_j$ is an average value of all samples. $S_B$ can be used to describe the distance between classes after performing projection on $W$:

\begin{align}
    (m_i - m)^2 &= (W^T \mu_i - W^T \mu)^2 \nonumber \\
                &= (W^T (\mu_i - \mu)(\mu_i - \mu)^T W \\
                &= W^T S_{B_i} W \nonumber
\end{align}

So, maximizing the nominator of the Fischer criterion results in the best linear separability after projection to lower-dimensional space.
$S_{W}$ is defined as:

\begin{equation}
    S_{W} = \sum_{i} A_{i}A_{i}^T
    \label{eq:SW}
\end{equation}

where $A_i$ is matrix of all samples $X_i$ from class $i$ reduced by $\mu_i$. $S_W$ is used to describe a distance between samples of the chosen class and its mean after projection:

\begin{align}
    (W^{T} X_i - M_i)^2 &= (W^{T} X_i - W^{T} U_i)^2 \nonumber \\
                        &= W^{T} (X_i - U_i)(X_i - U_i)^T W \\
                        &= W^{T} S_{Wi} W \nonumber
\end{align}

where $U$ is a matrix of the same size as $X_i$ and all columns equal to $\mu_i$. Thus minimizing denominator of Fischers criterion causes an increasement of density samples from the same class. This enables easier linear separability.

Equation [\ref{eq:Fischer_criterion}] can be also formulated as:

\begin{equation}
    S_{W}^{-1} S_{B} W = \lambda W
\end{equation}

This corresponds to finding eigenvalues and eigenvectors of $S_{W}^{-1} S_{B}$ matrix. Choosing $D$ eigenvectors with the largest corresponding eigenvalues as a base of the new space $\mathbb{R}^D$ guaranties good linear separability between classes. 

\subsection{Convolutional neural networks}

Convolutional neural network is a special type of neural network that is mainly based on the mathematical operation called convolution and the mechanism of pooling. Convolution in general is a linear operation on two functions of a real valued arguments. Let's assume that $x(t)$ is a function output of some operation that is performed on data $t$, which is usually called input and $w(t)$ is another operation that is called kernel. $s(t)$ is a convolution output that is sometimes referred as a feature map. Then we can define convolution as below:

\begin{equation}
    s(t) = (x*w)(t) = \int x(a)w(t-a)da
\end{equation}

In case of feature extraction CNNs mostly operate on discrete data, so discrete convolution can be defined as:

\begin{equation}
    s(t) = (x*w)(t) = \sum_{\alpha=-\infty}^{\infty} x(a)w(t-a)
\end{equation}

For two-dimensional data, like image I and kernel K we define convolution as:

\begin{equation}
    s(i, j) = (I*K)(i, j) = \sum_m \sum_n I(i-m, j-n)K(m, n)
\end{equation}

According to \cite{Goodfellow-et-al-2016}, convolution operation is used in the first of  the three main stages of convolutional layer. This stage produces a set of linear activations by performing several convolutions in parallel. Second stage, sometimes called the detector stage, is responsible for passing linear activations through a nonlinear activation function, such as the rectified linear unit activation function (ReLU). The last stage is based on a pooling function, which modifies the output of layer further. This function replaces the network output at certain location with a summary statistic of the nearby outputs. In some cases convolution, detector, and pooling stages are considered as separated network layers.

\section{Experimental settings}

\subsection{Data sets description}

\textsl{No free lunch theorems for Optimization} \cite{no-free-lunch} states that there is no general algorithm suited to solve all problems. Machine learning algorithms accuracy may depend on the application and the character of used data \cite{alppaydin}. For that reason datasets containing faces were chosen in order to perform comparison of selected extraction methods. These kind of datasets contain a lot of samples. Each sample contain many features, what suits the criterion of broad and deep datasets. Each pixel in an image can be considered as a separate dimension. Chosen datasets are compared in table \ref{tab:datasets}.

\begin{table}[!h]
    \centering
    \caption{Comparison of datasets.}
    \begin{tabular}{|c|c|c|c|}
         \hline
          \rowcolor{Gray}
         dataset & \thead{number of\\ images} & \thead{approximate\\ number of\\ images per subject} & source \\
         \hline
         specs-on-faces & 42592 & >100 & \cite{afifi2017afif4} \\
         \hline
         jaffe & 213 & 20 & \cite{JAFFE} \\
         \hline
         caltec & 450 & 16 & \cite{CaltechFaces} \\
         \hline
         georgia & 750 & 15 & \cite{georgia_tech_face_database} \\
         \hline
         umist & 576 & 30 & \cite{UMist-Faces} \\
         \hline
         stirling & 312 & 9 & \cite{Stirling_faces} \\
         \hline
         vidtimit & 94600 & >100 & \cite{VidTIMIT} \\
         \hline
         muct & 3755 & 15 & \cite{Milborrow10} \\
         \hline
         yale & 330 & 22 & \cite{yale} \\
         \hline
         att & 400 & 10 & \cite{TheDatabaseOfFaces} \\
         \hline
         mit  & 2000 & >100 & \cite{FaceRecognitionDatabase} \\
         \hline
         essex & 7900 & 20 & \cite{essex} \\
          \hline
    \end{tabular}
    \label{tab:datasets}
\end{table}



All images in chosen datasets contain varying lightning conditions, emotion expressions, face angles, varying ethnicity, sex and age of the subjects. Pictures were converted to the grayscale before performing feature extraction. 

\newpage

\subsection{Experiment description}

Goal of experiment was to compare classification efectiveness for selected FE methods over multiple datasets. Accuracy was chosen as a main measure of effectiveness. PCA, LDA, CNN-based feature extraction methods and classifiers were trained using the same training set. Models were evaluated with 10-fold stratified cross validation, which guarantees that both training and test sets contain proportional number of images for each class. The results for each classifier were given as an fold-wise average of all results obtained via cross validation. Folds were drawn separately for different classifiers. 

Multilayer perceptron (MLP) and support vector classifier (SVC) were used for comparison of feature extraction methods. All classifiers hyperparameters were fixed during the whole experiment. SVC was used with $L_{2}$ norm and $C=1.0$ for penalty and hinge loss. Reaching error tolerance that equals to $10^{-4}$ or reaching 1000 iterations were stopping conditions for SVC. MLP used rectified linear unit activation functions and three hidden layers containing 300, 200 and 150 neurons respectively. Adam optimizer was employed with hyperparameters: $\beta_1 = 0.9$, $\beta_2 = 0.999$, and $\epsilon=10^{-8}$. Regularization term that was introduced as $L_2$ penalty was equal to $10^{-5}$. The initial learning rate value was $0.001$. MLP required reaching $200$ iterations or tolerance for the optimization that equals to $10^{-4}$ in order to stop.
All hyperparameters are commonly used as a standard set that works well in the most of cases. No fine-tuning of hyperparameters of classifiers was performed.

Analysis of results was made using Friedman test with Nemenyi post-hoc test, as it was suggested in \cite{demsar}.

\subsection{Used hardware and tools}

Algorithms were compared on the Lenovo T470P using 7th generation Intel Core i7-7700HQ CPU and Nvidia GeForce 940MX GPU. Machine has 16GB of random access memory, which was used during the experiment. Implementations were written in Python 3.6. GPU-powered Keras implementation of CNN and scikit-learn implementations of LDA and PCA were used to perform tests. Image operations during preprocessing were implemented with OpenCV 3 library. SVM and MLP classifiers implementations were imported from scikit-learn. Statistical analysis was done using \textsl{numpy}, \textsl{scipy.stats} module, and implementation of Nemenyi post-hoc test that can be found in \cite{post_hoc_nemenyi}.

\subsection{Hyperparameters of FE methods}

For PCA and LDA a decomposition with 100 components was prepared. Common aproach for PCA is to take components that are responsible for 95\% of variance in the dataset. This approach can give better results in some cases, but it is very computationally expensive, especially for 10-fold cross validation. 

CNN architecture was based on 2 convolution layers with 7x7 and 3x3 kernels respectively, 2 pooling layers with 2x2 kernels and a dense layer. Convolution layers contained rectified linear unit  activation functions. Dense layer used ReLU activation functions. Last layer used Softmax function and had 100 outputs describing 100 components respectively.


\section{Results}

Tests were performed separately for SVC and MLP classifiers for all FE methods. Folds for SVC and MLP were drawn independently and significance tests for both classifiers were performed on different folds. Obtained accuracy and training time were presented in below tables and plots.

The Nemenyi test ranks performance of algorithms for each dataset, then calculates the mean of ranks for them. In the next step means are tested using Tukey test (to be more precise: differences of means are compared to the standard error) to check if there is a significant difference in performance over multiple datasets. In case of 3 data sets there are only 3 ranks that can be obtained. Each rank for chosen algorithm and dataset can take value of 1, 2 or 3, therefore difference between mean ranks will not be a large value (in fact maximum difference between mean ranks is 2). Significant difference will be noted by Nemenyi test between two algorithms which achieve ranks 1 or 3. This is a clear limitation of a post-hoc analysis in this case. Suppose we have two algorithms with a good performance (A and B) and one algorithm with a poor performance (C). Algorithm A is slightly better then algorithm B, therefore it obtains rank 3 more frequently. In this case due to comparing only average ranks significant difference between algorithms A and C will be reported, and no significant difference will be reported for algorithms B and C. This was taken into consideration in the further analysis of results.

During experiment an important issue with Stirling faces database was found. With about 9 images per class 10-fold stratified cross validation cannot be performed. $N$ fold stratified cross validation requires at least $N$ images per class. To overcome this problem Stirling database was oversampled.

\subsection{Results for SVC}


\begin{table}[!h]
    \centering
    \caption{Comparison of average accuracy of SVC classifier for chosen FE methods and datasets}
    \input{tables/Svm_acc_table.tex}
    \label{table:svm_acc_comparison}
\end{table}

\begin{table}[!h]
    \centering
    \caption{Comparison of average time of training SVC classifier for chosen FE methods and datasets}
    \input{tables/Svm_fit_time_table.tex}
    \label{table:svm_fit_time_comparison}
\end{table}

\begin{table}[!h]
    \centering
    \caption{Comparison of p-values and F-values for Friedman test for SVC classifier}
    \input{tables/Svm_pvalues.tex}
    \label{table:svm_pvalues}
\end{table}

\begin{table}[!h]
    \centering
    \caption{P-values of Nemenyi post-hoc tests for SVC classifier.}
    \begin{tabular}{|c|c|c|c|}
         \hline
          \rowcolor{Gray}
          & CNN & LDA & PCA \\
         \hline
         CNN &  1 &  0.128 &  0.003 \\
         \hline
         LDA &  0.128 & 1 &  0.381 \\
         \hline
         PCA &  0.003 & 0.381 & 1 \\
         \hline
    \end{tabular}
    \label{tab:svm_posthoc_pvalues}
\end{table}

\begin{figure*}[!h]
    \centering
    \includegraphics[scale=0.875]{images/Svm_accuracy_comparison.png}
    \caption{Comparison of average accuracy of SVC classifier for chosen FE methods and datasets}
    \label{fig:svm_acc_comparision}
\end{figure*}

\begin{figure*}[!h]
    \centering
    \includegraphics[scale=0.875]{images/Svm_fit_time_comparison.png}
    \caption{Comparison of average time of training SVC classifier for chosen FE methods and datasets}
    \label{fig:svm_fit_time_comparision}
\end{figure*}

\newpage

With obtained results null hypothesis of Friedmann test was rejected at a significance level of $\alpha = 0.05$ (see Table \ref{table:svm_pvalues}). 
Results of post-hoc Nemenyi tests for FE methods similarity were shown in Table \ref{tab:svm_posthoc_pvalues}. 
Average ranks obtained in post-hoc testing for CNN, LDA and PCA were correspondingly equal to: 2.708, 1.917 and 1.375.
Significant difference in performance was noted between CNN and PCA.
P-value for a difference between CNN and LDA was lower then for LDA and PCA, what might be confusing. This is related to the problem with Nemenyi test that was mentioned in the beginning of this section. For some datasets PCA obtained better accuracy then LDA, or even CNN. The difference in ranks between average ranks  was 0.791 for CNN and LDA and it was 0.542 for CNN and LDA. Bigger difference in average ranks means lower probability that the difference is lower then the standard error, therefore a p-value is lower. 

\newpage

\subsection{Results for MLP}


\begin{table}[!h]
    \centering
    \caption{Comparison of average accuracy of MLP classifier for chosen FE methods and datasets}
    \input{tables/NN_acc_table.tex}
    \label{table:NN_acc_comparison}
\end{table}

\begin{table}[!h]
    \centering
    \caption{Comparison of average time of training MLP classifier for chosen FE methods and datasets}
    \input{tables/NN_fit_time_table.tex}
    \label{table:NN_fit_time_comparison}
\end{table}

\begin{table}[!h]
    \centering
    \caption{Comparison of p-values and F-values for Friedman test for MLP classifier}
    \input{tables/NN_pvalues.tex}
    \label{table:NN_pvalues}
\end{table}

\begin{table}[!h]
    \centering
    \caption{P-values of Nemenyi post-hoc tests for MLP classifier.}
    \begin{tabular}{|c|c|c|c|}
         \hline
          \rowcolor{Gray}
          & CNN & LDA & PCA \\
         \hline
         CNN &  1 &  0.438 &  0.002 \\
         \hline
         LDA &  0.438 & 1 &  0.081 \\
         \hline
         PCA &  0.002 & 0.081 & 1 \\
         \hline
    \end{tabular}
    \label{tab:NN_posthoc_pvalues}
\end{table}

\begin{figure*}[!h]
    \centering
    \includegraphics[scale=0.875]{images/NN_accuracy_comparison.png}
    \caption{Comparison of average accuracy of MLP classifier for chosen FE methods and datasets}
    \label{fig:NN_acc_comparision}
\end{figure*}

\begin{figure*}[!h]
    \centering
    \includegraphics[scale=0.875]{images/NN_fit_time_comparison.png}
    \caption{Comparison of average time of training MLP classifier for chosen FE methods and datasets}
    \label{fig:svm_fit_time_comparision}
\end{figure*}

\newpage

According to the obtained results null hypothesis of Friedmann test was rejected at a significance level of $\alpha = 0.05$ (see Table \ref{table:NN_pvalues}). 
Results of a post-hoc Nemenyi tests for FE methods similarity were shown in Table \ref{tab:NN_posthoc_pvalues}. 
Average ranks for post hoc testing for CNN, LDA and PCA are equal to: 2.625, 2.125 and 1.25.
With $\alpha = 0.05$ null hypothesis was rejected for CNN and PCA.
Very low p-value was reported for LDA and PCA. This value is in line with obtained difference in average rank that equals 0.875.

\newpage


\section{Conclusions}

Experiments were carried out with established methodology. Stratified 10-fold cross validation proved itself to be a good choice for performing validation over multiple datasets. During experiments FE algorithms were trained on the same training set as the classifier in order to achieve separation of training and testing data for both FE methods and for classifiers. Due to this restriction experiments were computionally expensive, but they provided more reliable results. Validation can be further improved by using 10-fold cross validation repeated 10 times with averaging obtained results. This idea was not used in prepared experiment because of computational power limitations. Usage of a scikit-learn cross validation allowed to save time by not writing custom implementations.

Results were collected and analysed. For both SVC and MLP classifiers CNN yielded better performance than PCA and LDA. No statistical difference was noted between LDA and PCA. Some problems for PCA combined with SVC can be noted for few datasets, especially for datasets with different face orientations (e.g. att, georgia, muct or stirling). Obtained post hoc tests p-values are intuitively in step with an average accuracy for all datasets. This indicates that statistical methods employed for verification were suitable for analysed problem. There is also a low chance that reported results of statistical analysis are not valid.

Training time was reported only for illustrative purposes. No statistical analysis of training time was performed. One may notice here that training time of SVC with CNN FE for 8 out of 12 databases was longer then for other FE algorithms. Also training time of MLP for CNN FE for 10 out of 12 databases was longer then for PCA and LDA.

In the future experiments CNN can be compared with other FE methods. Also other choice of datasets could provide different results. Analysis of training time can be performed, but it could be difficult to estimate reliability of the results. This is due to many factors that can influence such experiment (e.g. various implementations of algorithms, operating systems, processor architecture, optimisations introduced by virtual machine, etc). 


\bibliographystyle{plabbrv}
\bibliography{bibliography}

\end{document}